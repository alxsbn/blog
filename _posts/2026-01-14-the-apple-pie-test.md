---
layout: post
title: "The Apple Pie Test"
date: 2026-01-14
categories: ai philosophy work series
excerpt: "A consultant hid cooking recipes in technical documentation. No one ever noticed. AI agents would—but who decides what they look for?"
header_image: "https://images.unsplash.com/photo-1568667256549-094345857637?w=1600&q=80"
header_image_alt: "Library card catalog with wooden drawers and index cards"
header_image_credit: "Jan Antonin Kolar"
header_image_credit_url: "https://unsplash.com/@jankolar"
header_image_source: "Unsplash"
header_image_source_url: "https://unsplash.com"
---

Years ago, I met a Z-System guru who was working for IBM during a mission at a bank. The man had spent decades writing technical documentation for mainframe systems. Thousands of pages that the client paid handsomely for.

His trick? He buried cooking recipes in the middle of it. Apple pie. Beef bourguignon. Crêpes.

Most of the time, no one ever mentioned them.

The documentation sat on shelves, referenced in contracts, cited in audits, but never actually *read*. The recipes were his private proof of a public lie: everyone pretended the work had been reviewed.

## The universal cheat

This wasn't fraud. It was the normal functioning of organizations. The client paid for documentation because having it was required. Reading it was not. The consultant delivered because that's what the contract said. Whether anyone engaged with the content was beside the point.

Both sides knew the game. The apple pie was just making the implicit explicit.

This is everywhere. Reports written to be filed, not read. Meetings held to be logged, not for decisions. Compliance checklists checked without checking. The gap between *prescribed work* and *real work* is held together by a conspiracy of mutual pretense.

Everyone cheats. Everyone knows everyone cheats. The system runs anyway.

## Enter the agent

An AI agent can't play this game.

Ask it to review documentation, and it will actually review it. Every page. It would find the apple pie recipe on page 847 and flag it: *"This section appears to contain unrelated culinary content. Should this be removed?"*

The agent is not smarter. It's just incapable of the wink.

It doesn't know that some documents exist to exist. That some processes are theater. That the point of certain deliverables is to have been delivered, not to be used. It takes the prescription at face value.

This is its honesty. And it's devastating.

## What the recipe revealed

The Z-System guy wasn't just testing whether people read. He was measuring the gap between what organizations say they do and what they actually do.

That gap is where work happens. Where judgment lives. Where humans decide—collectively, implicitly—what matters and what doesn't. The apple pie test revealed that documentation review *didn't matter*. Not really. Whatever the official process said.

But here's the thing: this only works if everyone is human. If everyone can read the room. If everyone understands that some rules exist to be bypassed.

An AI agent reads the document, not the room.

## Honesty as disruption

When the agent finds the recipe, it doesn't just flag a mistake. It exposes the cheat. It makes visible the informal agreement that kept the system stable.

Suddenly, the client has to answer: *Why wasn't this caught before?* The answer will be "because no one ever reads these"—true but unspeakable. So instead there's blame, process revision, new controls. More theater to paper over the exposed gap.

The agent didn't cheat. And in not cheating, it broke something that worked.

## The new asymmetry

We're entering a world where agents are incapable of the small lies that lubricate organizations. They can't pretend to have read what they skipped. They can't nod along to a requirement they know is absurd. They can't play the game of mutual pretense.

But humans still can. And still must.

The question becomes: who manages the translation? Who teaches the agent which rules are real and which are ritual? Who decides when perfect compliance would be organizational suicide?

The guru could hide recipes because he understood which documents were decorative. That knowledge was tacit, earned, unteachable. And now it's more valuable than ever—because someone needs to protect the organization from its own honest machines.

## But honest to whom?

Here's what bothers me about the "AI is honest" narrative: the agent can't lie about what it finds. But it only finds what it's pointed at.

The apple pie recipe would be caught instantly. But who decides which documents get reviewed? Who writes the prompt? Who defines what counts as a "discrepancy" worth flagging?

The old cheat was symmetric. The consultant pretended to deliver useful documentation. The client pretended to read it. Both sides knew. Both sides benefited from not knowing officially.

The new world is asymmetric. The warehouse worker's movements are tracked to the second. The manager who configured that tracking operates in strategic ambiguity. The customer service agent's every response is evaluated for compliance. The executive who set the compliance rules is not.

The honest machine creates honest workers. It doesn't necessarily create honest organizations.

## The recipe, relocated

Somewhere, in some archive, there's still a mainframe manual with instructions for apple pie between the JCL syntax and the VSAM specifications.

It sits there as proof: systems have always run on selective blindness. On the agreement to not see what doesn't need to be seen.

AI agents can't hide recipes in documentation anymore. But you can hide the equivalent in their instructions—biases in prompts, exceptions in configurations, blind spots in scope definitions.

The agent can't cheat. But it can be aimed. And aiming is the new cheating—just for those who hold the compass.
