---
layout: post
title: "The New Stakhanovism"
date: 2026-02-09
categories: ai work philosophy organization series
excerpt: "Same fable, different factory."
header_image: "https://upload.wikimedia.org/wikipedia/commons/thumb/f/f1/Stakhanov.JPG/1600px-Stakhanov.JPG"
header_image_alt: "Alexei Stakhanov, Soviet coal miner and hero of the Stakhanovite movement"
header_image_credit: "Soviet Archives"
header_image_credit_url: "https://commons.wikimedia.org/wiki/File:Stakhanov.JPG"
header_image_source: "Wikimedia Commons"
header_image_source_url: "https://commons.wikimedia.org"
ref: the-new-stakhanovism
redirect_from:
  - /2026/02/09/the-new-stakhanovism/
lang: en
---

## The fable

In 1935, Alexeï Stakhanov extracted 102 tonnes of coal in a single shift—14 times the norm. The Soviet Union made him a hero. The Stakhanovite movement was born: model workers surpassing quotas, proving the system worked.

What the propaganda didn't mention: teams cleared the way before him, removed obstacles, prepared his tools. He wasn't alone. He was *staged*.

The staging served two purposes. Legitimize the system. And raise quotas for everyone else—"if Stakhanov can do it, why can't you?"

In June 2025, Elena Verna, Head of Growth at Lovable, publishes [The Rise of the AI-Native Employee](https://www.elenaverna.com/p/the-rise-of-the-ai-native-employee). Small teams, no process, no handoffs, no middle management. Default to AI. Build, don't coordinate. 35 people for $80M ARR. The #AINativeEmployeeEra.

Eight months later, Lovable puts Lazar Jovanovic on [Lenny's Podcast](https://www.lennysnewsletter.com/p/getting-paid-to-vibe-code)—the largest product podcast in tech. He's their "first professional vibe coder." No coding background. Ships production-quality products using only AI. The thesis made flesh.

Meanwhile, Lovable has 47 open positions and ~350 employees. GTM Operations. Recruitment Coordinator. Customer Success Manager. Enterprise Account Executive. Security Engineer with "5+ years in cloud-native environments." Analytics Engineer supporting Sales and Customer Success. Almost word for word, the roles Elena described as going extinct.

Lazar is one person out of 350. Behind his demos, there's infrastructure maintained by platform engineers, security handled by specialists, enterprise deals closed by experienced AEs, and a GTM machine staffed by seasoned operators. Lovable is the most visible case of a broader pattern. Klarna replaced 700 customer service agents with AI and made it a press release. Duolingo cut contractors after expanding its AI capabilities and framed it as evolution. The fable is always the same: the system works, the human is optional. Lovable tells it loudest—and the gap between its narrative and its job board is the most verifiable in real time.

This doesn't refute the thesis. A comparable SaaS at this stage might employ a thousand people. Lovable may genuinely be building a leaner model. But there's a gap between a transition and a fable. When you scale to 350 while telling the story of 35, you're not describing the future. You're curating it.

The parallel isn't political. Nobody went to the gulag. It's *narrative*. Same hero structure, same function: a model worker made visible, an infrastructure made silent, and a norm set for everyone else.

## The zeal

Christophe Dejours, psychiatrist and founder of the psychodynamics of work, describes what he calls [living work](/en/2026/01/01/the-cheating-that-makes-the-world-run/): the moment where the worker's body encounters the resistance of the real. A worker on an assembly line. The bolt doesn't fit into the thread. It's not in the procedure. The line keeps moving. So he dips the bolt in a bucket of oil, forces it in by hand, falls behind—then catches up. The product ships. No one knows.

The deviation *is* the work. And the deviation is a form of zeal—not obedience to the instruction, but loyalty to the outcome. This is productive zeal. The kind that says: I know what was asked, but here's what actually needs to happen.

There's another kind. The unnecessary meeting convened out of process loyalty. The political workaround. The cargo cult review. The weekly status report no one reads. This is parasitic zeal—devotion to the ritual of work rather than its substance.

An AI agent strips both in the same gesture. It has no body, no friction with the real. When the bolt doesn't fit, it escalates, blocks, or hallucinates a solution. It doesn't dip anything in oil. It doesn't *cheat*. Modern agentic systems are sophisticated—fallback rules, contextual escalation, multi-step reasoning. But every deviation an agent makes was anticipated by its designer. The bolt in the oil wasn't. Productive zeal, by definition, exceeds what anyone pre-configured. That's what makes it productive. But it also doesn't convene pointless meetings or navigate office politics. It removes productive deviance and parasitic deviance identically, because from inside a system prompt, they look the same.

"No process, no handoffs, just build" is the [work-to-rule](/en/2026/01/01/the-cheating-that-makes-the-world-run/) of AI—not obedience to slow things down, but obedience to speed things up. In both cases, the space where human judgment corrects the gap between instruction and reality gets compressed.

When the agent automates everything, who is the guarantor of zeal? Not the parasitic kind. The productive kind. The kind that senses the bolt doesn't fit before the product ships.

Someone has to decide what the agent knows, what it ignores, and where it stops. Someone configures the frame. And that someone may already be the most important person in the building.

## The compression

The Stakhanovite movement didn't just celebrate productivity. It served a specific economic function: raising quotas. If Stakhanov could do 14× the norm, the norm could go up. Every worker was now measured against the model. The productivity wasn't redistributed—it was *extracted*.

The AI-native narrative runs the same logic with different packaging. When a company demonstrates that one person with AI can do the work of four, two things can happen. Produce four times more. Or fire three out of four.

Both happen. But the dominant narrative in the AI-native discourse rewards extraction and invisibilizes redistribution. The model worker goes on a podcast. The people whose roles became "redundant" update their LinkedIn in silence.

This is the compression. Not the compression of execution time—that part is real, and often welcome. The compression of the workforce into a tighter, faster, more surveilled unit where the norm has been reset by a demo on a podcast. Where "if Lazar can do it" quietly becomes "why do we need you?"

And the person who configures the agent that sets the pace? That person decides, in practice, what "normal productivity" looks like. Not by setting quotas. By setting context.

## The banality of configuration

Look at Grok. The responses aren't neutral-by-default with accidental bias. They're *configured*. Someone decided what the agent treats as fact, what it treats as controversy, what it refuses to engage with. Someone encoded a worldview into the architecture—and that someone is neither elected, nor visible, nor accountable.

Now transpose this inside your company.

Every organization deploying an AI agent—Copilot, Glean, a custom internal assistant—faces the same invisible decision. Someone defines what the agent knows about the company. What it surfaces. What it omits. What it refuses to answer. What tone it uses. What it treats as settled and what it treats as open.

An employee asks the internal agent about reorganization plans. About compensation benchmarks. About why a project was killed. The answers—or the refusals to answer—aren't neutral. They're configured. By someone.

This person is not the CTO. Not the Chief Ethics Officer. Not the Data Protection Officer. In most organizations, this person doesn't even have a title. The role is exercised de facto, buried in engineering, treated as a technical implementation detail.

It is not a technical implementation detail.

When you configure the boundaries of an AI agent that every employee interacts with daily, you are designing the cognitive architecture of the organization. You're deciding what's thinkable.

Hannah Arendt identified a specific mechanism: the diligent functionary who executes without reflecting on the finality of what he executes. Not a monster. A bureaucrat. Someone who "does engineering" while actually doing ideology. The parallel isn't moral—it's structural. Call it the banality of configuration: a technical role that shapes organizational reality without ever being asked to account for it.

A company is, by structure, a small managed authority. The context manager is the person who designs the cognitive architecture of that authority—the one who builds the machine that shapes how everyone else thinks about the organization they work in. This role has no precedent. No governance. No audit trail.

The fact that this role doesn't have a formal name is not a gap in org charts. It's the gap. Call it Chief Context Officer. The role is being exercised right now, in every company with an internal AI deployment, by someone whose power is inversely proportional to their visibility. Naming it doesn't create it. Naming it reveals that it already exists, ungoverned.

## What remains open

The Soviet system collapsed not because it failed, but because it [succeeded too well](/en/2026/01/04/collapse-through-obedience/)—on paper. Perfect reports, zero truth. The gap between the prescribed and the real grew until the system could no longer see itself.

The organizations that will navigate this transition won't be the fastest, nor the most nostalgic. They'll be the ones that understood two things.

That productive zeal—the bolt in the oil, the deviation that saves the product, the judgment that no system prompt can encode—cannot be automated, only crushed. And that the difference between the slack that wastes and the slack that saves is blurrier than any optimization framework can see.

And that someone, right now, is configuring the machine that configures everyone else. Without a title. Without oversight. Without anyone asking what the configuration produces.

The next organizational scandal won't come from a CEO. It will come from an engineer whose title nobody knew.
