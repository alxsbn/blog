---
layout: post
title: "The New Stakhanovism"
date: 2026-02-16
categories: ai work philosophy organization series
excerpt: 'The Stakhanovite movement celebrated model workers to raise quotas for everyone else. AI-native companies are running the same playbook.'
header_image: "https://upload.wikimedia.org/wikipedia/commons/thumb/f/f1/Stakhanov.JPG/1600px-Stakhanov.JPG"
header_image_alt: "Alexei Stakhanov, Soviet coal miner and hero of the Stakhanovite movement"
header_image_credit: "Soviet Archives"
header_image_credit_url: "https://commons.wikimedia.org/wiki/File:Stakhanov.JPG"
header_image_source: "Wikimedia Commons"
header_image_source_url: "https://commons.wikimedia.org"
ref: the-new-stakhanovism
redirect_from:
  - /2026/02/09/the-new-stakhanovism/
  - /en/2026/02/09/the-new-stakhanovism/
lang: en
---

Alexei Stakhanov was a coal miner. Lazar Jovanovic is a vibe coder. Ninety years apart, but the mechanism is the same: a model worker, an invisible infrastructure, and quotas that rise for everyone.

## The fable

In 1935, Alexei Stakhanov extracted 102 tonnes of coal in a single shift, 14 times the norm. The Soviet Union made him a hero, and the Stakhanovite movement was born. Model workers surpassed quotas, proving the system worked.

What the propaganda didn't mention were the teams that cleared the way before him, removed obstacles, and prepared his tools. He wasn't alone, he was *staged*.

The staging served two purposes: legitimize the system, and raise quotas for everyone else. "If Stakhanov can do it, why can't you?"

In June 2025, Elena Verna, Head of Growth at Lovable, publishes [The Rise of the AI-Native Employee](https://www.elenaverna.com/p/the-rise-of-the-ai-native-employee). She describes a radical model: small teams with no process, no handoffs and no middle management, where AI is the default tool. Build, don't coordinate, with 35 people for $80M ARR. The #AINativeEmployeeEra.

Eight months later, Lovable puts Lazar Jovanovic on [Lenny's Podcast](https://www.lennysnewsletter.com/p/getting-paid-to-vibe-code), the largest product podcast in tech. He's their "first professional vibe coder": no academic background, he ships production-quality products using only AI. Lazar is not an engineer, and that's the thesis made flesh.

Meanwhile, Lovable lists dozens of open positions, including Recruitment Coordinator, Customer Success Manager, and Enterprise Account Executive. Exactly the coordination and middle management roles Elena described as going extinct.

Behind Lazar's demos, there are platform engineers, security specialists, and experienced account executives. But the fable retains one thing: if he can do it, why can't you?

Lovable is the most visible case of a broader pattern. Amazon [cut 18,000 jobs](https://www.npr.org/2023/01/04/1147034858/amazon-ceo-says-company-will-layoff-more-than-18-000-workers) in the United States. Klarna announced that [its AI assistant does the work of 700 agents](https://www.klarna.com/international/press/klarna-ai-assistant-handles-two-thirds-of-customer-service-chats-in-its-first-month/). Duolingo [cut its contractors](https://techcrunch.com/2024/01/09/duolingo-cut-10-of-its-contractor-workforce-as-the-company-embraces-ai/) after expanding its AI capabilities and framed it as evolution. The fable is always the same: the system works, and the human is optional. The Hacker News discussions ([Amazon](https://news.ycombinator.com/item?id=34427277), [Klarna](https://news.ycombinator.com/item?id=39531662), and [Duolingo](https://news.ycombinator.com/item?id=38918279)) are sometimes more revealing than the press releases themselves.

This doesn't refute the thesis, a comparable SaaS at this stage might employ a thousand people, and Lovable may genuinely be building a leaner model. But when a company's actual headcount diverges that far from the narrative it projects, you're not describing the future, you're curating it.

And the parallel is obviously not political, nobody went to the gulag. It's *narrative*: a model worker made visible, an infrastructure made silent, and a norm set for everyone else.

## The zeal

But the fable only works if we forget what it compresses. And what gets compressed first is the nature of human work itself.

Take a worker on an automotive assembly line. He needs to screw a bolt into a thread. The bolt doesn't fit. He tries a second one, then a third, none of them go in. The procedure says to stop the line. But stopping the line costs tens of thousands of euros per minute. So he runs to grab a bucket of oil, dips the bolts in, and forces them in one by one. The line doesn't stop. The product ships, and no one knows.

This worker, this gap between the procedure and the act, that's [living work](/en/2026/01/01/the-cheating-that-makes-the-world-run/). Not obedience to the instruction but loyalty to the outcome. This is productive zeal, the kind that says: I know what was asked, but here's what actually needs to happen.

There's another kind: the unnecessary meeting convened out of process loyalty, the political workaround, the ritual code review no one actually reads, and the [weekly status report that disappears into the void](/en/2026/01/23/the-apple-pie-test/). This is parasitic zeal, devotion to the ritual of work rather than its substance.

An AI agent strips both in the same gesture. It has no body, no friction with the real. It doesn't dip anything in oil, it doesn't *cheat*, but it also doesn't convene pointless meetings. From inside a system prompt, the two deviances look the same.

"No process, no handoffs, just build" is the [work-to-rule](/en/2026/01/01/the-cheating-that-makes-the-world-run/) of AI, not obedience to slow things down but obedience to speed things up. In both cases, the space where human judgment corrects the gap between instruction and reality gets compressed.

## The compression

When the space for judgment compresses, the norm rises. The Stakhanovite movement served a specific economic function: if Stakhanov could do 14x the norm, the norm could go up. Every worker was measured against the model, and the productivity wasn't redistributed, it was *extracted*.

The AI-native narrative runs the same logic. When a company demonstrates that one person with AI can do the work of four, the question isn't whether to produce four times more, it's why you're still paying the other three. The model worker goes on a podcast, while the people whose roles became "redundant" update their LinkedIn in silence.

This is [stack ranking](https://en.wikipedia.org/wiki/Vitality_curve) logic applied not by a manager during annual reviews but by a podcast appearance. The person who uses AI effectively becomes the benchmark. Everyone develops their own formula, their own setup to produce beyond what the old pace allowed, and expectations rise. At some point, a choice will have to be made: the new Stakhanov will be preferred over the other. The model worker recalibrates the norm for everyone else.

In this context, Lazar takes the question to its limit. He has no academic background. If you take him seriously, what's the remaining argument for the certified engineer stacking increasingly complex architectures?

## What remains open

The original Stakhanovism collapsed under the weight of its own quotas. Productive zeal (the bolt in the oil, the deviation that saves the product, and the judgment that no system prompt can encode) cannot be automated, only crushed.

One question remains that no one is asking. Someone has to decide what the agent knows, what it ignores, and where it stops. That someone configures the boundaries of an AI agent that every employee interacts with daily, deciding what's thinkable. This role has no name, no governance, and no audit trail. Call it, echoing Hannah Arendt, the banality of configuration. It's [a question of power, not technique](/en/2026/02/18/who-writes-the-constitution-of-machines/).
